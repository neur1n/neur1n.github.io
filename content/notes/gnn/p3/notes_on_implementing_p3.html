<!DOCTYPE HTML>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/css/style.css">

  <title>Jihang Li - Notes on Implementing P3</title>

  <script src="/js/theme.js"></script>
  <script>
    listenToggle();
  </script>
</head>

<body>
  <object type="text/html" data="/header.html" class="header"></object>

  <div>
    <h1>Notes on Implementing P<sup>3</sup></h1>

    <script>
      document.write(document.lastModified);
    </script>

    <ul>
      <li><a href="#data-preprocessing">Data Preprocessing</a></li>
      <li><a href="#forward-pass">Forward Pass</tt></a></li>
    </ul>

    This note presumes that the reader has finished reading the
    <a href="https://docs.dgl.ai/tutorials/blitz/index.html">A Blitz Introduction to DGL</a>
    and the <a href="https://docs.dgl.ai/tutorials/dist/1_node_classification.html">Distributed Node Classification</a>
    documentations, and the contents are based on node classification only.

    <h2 id="data-preprocessing">Data Preprocessing</h2>
    <p>
      Before partitioning, it is crucial to ensure that the node labels in the
      DGL graph are stored for future usage:
    </p>

    <pre>
from dgl.distributed import partition_graph
from ogb.nodeproppred import DglNodePropPredDataset
import dgl

dataset = DglNodePropPredDataset("ogbn-arxiv")

graph, node_labels = dataset[0]
graph = dgl.add_reverse_edges(graph)

graph.ndata["label"] = node_labels[:, 0]
    </pre>

    <p>
      It is worth noting that the current documentation does not explicitly
      outline the necessity of this step. As a result, it is easy to overlook,
      potentially leading to issues when attempting to use code copied from
      DGL's documentation, as the absence of this step can impede its
      functionality.
    </p>

    <h2 id="forward-pass">Forward Pass</h2>
    <p>
      There are at least two ways to implement the forward pass as P<sup>3</sup>
      does, modifying DGL's built-in class or implement a new class from scratch.
    </p>

    <h3>Modifying DGL</h3>
    <p>
      To modify DGL's classes to enable the switching between model parallelism
      and data parallelism, it is essential to identify the appropriate points
      for inserting scatter and gather operations. Within DGL, the forward pass
      is accomplished by invoking the <tt>dgl.heterograph.DGLGraph.update_all</tt>
      function, which follows this call stack:
    </p>

    <pre>
dgl.heterograph.DGLGraph.update_all
|- dgl.core.message_passing
   |  |- dgl.core.invoke_gspmm:  if message and reduce are built-in
   |
   |                             else
   |---- dgl.core.invoke_gsddmm: message
         dgl.core.invoke_gspmm:  reduce
    </pre>

    <p>
      Re-implementing DGL's built-in message and reduce functions is a
      straightforward process, enabling us to consistently carry out the
      forward pass by invoking <tt>dgl.core.invoke_gsddmm</tt> and
      <tt>dgl.core.invoke_gspmm</tt>. The critical point for inserting scatter
      and gather functions lies between these two operations. This is where we
      can customize the behavior to seamlessly switch between model parallelism
      and data parallelism based on our requirements.
    </p>

    <h3>Implementing New Classes</h3>
    <p>
      Let us take a look at P<sup>3</sup>'s pipeline strategy (Figure 6 in paper).
    </p>

    <img src="/content/notes/gnn/p3/image/fiture_6.png" alt="figure 6" style="width:80%">

    <p>
      Upon a glance, an evident pattern emerges, excluding batches 1 and 2. The
      pipelining strategy consistently follows a systematic four-step sequence:
    </p>

    <ol>
      <li>batch<sub>n</sub>: Model parallelism forward pass.</li>
      <li>batch<sub>n-1</sub>: Data parallelism forward pass.</li>
      <li>batch<sub>n-1</sub>: Data parallelism backward pass.</li>
      <li>batch<sub>n-2</sub>: Model parallelism backward pass.</li>
    </ol>

    <p>
      Adhering to this distinct pattern, my current design takes shape:
    </p>

    <pre>
class SAGEConv(object):
    def __init__(self):
        self._job_thd = threading.Thread(target=self._handle_job)
        self._scatter_thd = threading.Thread(target=self._handle_scatter)
        self._gather_thd = threading.Thread(target=self._handle_gather)
        self._transform_thd = threading.Thread(target=self._handle_transform)
        self._sync_thd = threading.Thread(target=self._handle_sync)
        self._apply_thd = threading.Thread(target=self._handle_apply)

        self._jobs = [None, None, None]  # Intermediate data of 3 batches
        self._scatter_que = queue.Queue()
        self._gather_que = queue.Queue()
        self._transform_que = queue.Queue()
        self._sync_que = queue.Queue()
        self._apply_que = queue.Queue()
        self._output_que = queue.Queue()

  def forward(self, ...):
      # Push data (i.e., graph, features and other necessary data) to self._jobs.
      # Return result once there is a new data apperas in self._output_que.

  def _handle_job(self):
      # Get data from self._jobs and push them to the appropriate queues accordingly.

  def _handle_scatter(self):
      # Get data from self._scatter_que and perform scatter.

  def _handle_gather(self):
      # Get data from self._gather_que and perform gather.

  def _transform_gather(self):
      # Get data from self._transform_que and perform transform.

  def _handle_sync(self):
      # Get data from self._sync_que and perform synchronization.

  def _handle_apply(self):
      # Get data from self._apply_que and perform apply.
      # Then push the result to self._output_que.
    </pre>

    <p>
      Please visit the <a href="https://github.com/neur1n/p3" target="_blank">code</a>
      for implementation details. However, it is important to note that the
      backward pass process might necessitate a customized approach which I
      have not done yet.
    </p>
  </div>
</body>
</html>
