<!DOCTYPE HTML>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1">

  <title>Jihang Li - Notes on Implementing P3</title>

  <link rel="stylesheet" type="text/css" href="/css/style.css">

  <script src="/js/theme.js"></script>
  <script>
    listenToggle();
  </script>
</head>

<body>
  <object type="text/html" data="/header.html" class="header"></object>

  <div>
    <h1>Notes on Implementing P<sup>3</sup></h1>

    <script>
      document.write(document.lastModified);
    </script>

    <ul>
      <li><a href="#data-preprocessing">1 Data Preprocessing</a></li>
      <li><a href="#forward-pass">2 Forward Pass</tt></a></li>
      <li><a href="#problems">3 Problems</tt></a></li>
    </ul>

    <p>
      This note presumes that the reader has finished reading the
      <a href="https://docs.dgl.ai/tutorials/blitz/index.html">A Blitz Introduction to DGL</a>
      and the <a href="https://docs.dgl.ai/tutorials/dist/1_node_classification.html">Distributed Node Classification</a>
      documentations, and the contents are based on node classification only.
    </p>

    <h2 id="data-preprocessing">1 Data Preprocessing</h2>
    <p>
      Before partitioning, it is crucial to ensure that the node labels in the
      DGL graph are stored for future usage:
    </p>

    <div class="code-block"><!--
-->from dgl.distributed import partition_graph
from ogb.nodeproppred import DglNodePropPredDataset
import dgl

dataset = DglNodePropPredDataset("ogbn-arxiv")

graph, node_labels = dataset[0]
graph = dgl.add_reverse_edges(graph)

graph.ndata["label"] = node_labels[:, 0]<!--
 --></div>

    <p>
      It is worth noting that the current documentation does not explicitly
      outline the necessity of this step. As a result, it is easy to overlook,
      potentially leading to issues when attempting to use code copied from
      DGL's documentation, as the absence of this step can impede its
      functionality.
    </p>

    <h2 id="forward-pass">2 Forward Pass</h2>
    <p>
      There are at least two ways to implement the forward pass as P<sup>3</sup>
      does, modifying DGL's built-in class or implement a new class from scratch.
    </p>

    <h3>2.1 Modifying DGL</h3>
    <p>
      To modify DGL's classes to enable the switching between model parallelism
      and data parallelism, it is essential to identify the appropriate points
      for inserting scatter and gather operations. Within DGL, the forward pass
      is accomplished by invoking the <tt>dgl.heterograph.DGLGraph.update_all</tt>
      function, which follows this call stack:
    </p>

    <div class="code-block"><!--
-->dgl.heterograph.DGLGraph.update_all
|- dgl.core.message_passing
   |  |- dgl.core.invoke_gspmm:  if message and reduce are built-in
   |
   |                             else
   |---- dgl.core.invoke_gsddmm: message
         dgl.core.invoke_gspmm:  reduce<!--
 --></div>

    <p>
      Re-implementing DGL's built-in message and reduce functions is a
      straightforward process, enabling us to consistently carry out the
      forward pass by invoking <tt>dgl.core.invoke_gsddmm</tt> and
      <tt>dgl.core.invoke_gspmm</tt>. The critical point for inserting scatter
      and gather functions lies between these two operations. This is where we
      can customize the behavior to seamlessly switch between model parallelism
      and data parallelism based on our requirements.
    </p>

    <h3>2.2 Implementing New Classes</h3>
    <p>
      Let us take a look at P<sup>3</sup>'s pipeline strategy (Figure 6 in paper).
    </p>

    <img src="/content/notes/gnn/p3/image/fiture_6.png" alt="figure 6" style="width:80%">

    <p>
      Upon a glance, an evident pattern emerges, excluding batches 1 and 2. The
      pipelining strategy consistently follows a systematic four-step sequence:
    </p>

    <ol>
      <li>batch<sub>n</sub>: Model parallelism forward pass.</li>
      <li>batch<sub>n-1</sub>: Data parallelism forward pass.</li>
      <li>batch<sub>n-1</sub>: Data parallelism backward pass.</li>
      <li>batch<sub>n-2</sub>: Model parallelism backward pass.</li>
    </ol>

    <p>
      Adhering to this distinct pattern, my current design takes shape:
    </p>

    <div class="code-block"><!--
-->class SAGEConv(object):
    def __init__(self):
        self._job_thd = threading.Thread(target=self._handle_job)
        self._scatter_thd = threading.Thread(target=self._handle_scatter)
        self._gather_thd = threading.Thread(target=self._handle_gather)
        self._transform_thd = threading.Thread(target=self._handle_transform)
        self._sync_thd = threading.Thread(target=self._handle_sync)
        self._apply_thd = threading.Thread(target=self._handle_apply)

        self._jobs = [None, None, None]  # Intermediate data of 3 batches
        self._scatter_que = queue.Queue()
        self._gather_que = queue.Queue()
        self._transform_que = queue.Queue()
        self._sync_que = queue.Queue()
        self._apply_que = queue.Queue()
        self._output_que = queue.Queue()

  def forward(self, ...):
      # Push data (i.e., graph, features and other necessary data) to self._jobs.
      # Return result once there is a new data apperas in self._output_que.

  def _handle_job(self):
      # Get data from self._jobs and push them to the appropriate queues accordingly.

  def _handle_scatter(self):
      # Get data from self._scatter_que and perform scatter.

  def _handle_gather(self):
      # Get data from self._gather_que and perform gather.

  def _transform_gather(self):
      # Get data from self._transform_que and perform transform.

  def _handle_sync(self):
      # Get data from self._sync_que and perform synchronization.

  def _handle_apply(self):
      # Get data from self._apply_que and perform apply.
      # Then push the result to self._output_que.<!--
 --></div>

    <p>
      Please visit the <a href="https://github.com/neur1n/p3" target="_blank">code</a>
      for implementation details. However, it is important to note that the
      backward pass process might necessitate a customized approach which I
      have not done yet.
    </p>

    <h2 id="problems">3 Problems</h2>
    <h3>3.1 Training on Multiple Machines</h3>
    <p>
      When initiating a distributed training across multiple machines, an error
      consistently emerged, displaying the message "Cannot assign requested
      address." After conducting some research, the problem might arise from
      address unavailability due to
    </p>

    <ul>
      <li>Port exhaustion.</li>
      <li>Conflicting with processes occupying the desired addresses.</li>
      <li>An excessive frequency of binding.</li>
    </ul>

    <p>
      Regarding the port concern, the <code>ip_local_port_range</code> default
      setting spans from 23768 to 60999. Despite attempting to expand this
      range, the issue persisted unchanged. It's worth noting that a port
      shortage nor address accessing conflict seemed improbable since a
      thorough examination using <code>netstat -a</code>.
    </p>

    <p>
      In terms of binding requests, the distributed training configuration
      involved initiating 1 server and 1 client on each of the 2 machines,
      resulting in a total of 2 instances. While the frequency of binding
      requests was not inherently expected to be excessive, a definitive
      assessment of their impact remained uncertain.
    </p>

    <p>
     In pursuit of a solution, I also explored using LXC containers and virtual
     machines on my Ubuntu 22.04 system to replicate a cluster-like
     environment. Regrettably, despite these efforts to mitigate potential
     hardware misconfigurations, the persistent error remains unchanged.
    </p>

    <h3>3.2 Training on Single Machine</h3>
    <p>
      According to this <a href="https://discuss.dgl.ai/t/partition-policy-nodeh-not-found-in-dist-graphsage-model/2023/4" target="_blank">comment</a>,
      the original design of DGL did not account for distributed training
      involving more than one graph partition on a single machine. A detailed
      exploration of DGL's source code brought to light one, albeit potentially
      not the exclusive, reason for this constraint is that DGL employs a
      combination of the graph name and node/edge type for naming shared
      memories. Consequently, a limitation arises wherein subsequent node
      initiations can inadvertently overwrite shared memory containing data
      from previously initiated nodes. This issue becomes particularly relevant
      when multiple training nodes are launched on a single machine.
    </p>

    <p>
      To address the shared memory conflict, I took the initiative to modify
      DGL's naming methods and experimented with the execution of multiple
      servers on a single machine. This modification successfully mitigated the
      shared memory issue. However, a new challenge emerged when constructing
      the <code>dgl.distributed.DistGraph</code>. In this scenario, a recurring
      occurrence was the assignment of incorrect partition metadata (e.g.,
      assigning client 0 with partition 1). This resulted in an inaccurate node
      ID mapping and, inevitably, triggered out-of-bounds indexing of the
      underlying data.
    </p>
  </div>
</body>
</html>
