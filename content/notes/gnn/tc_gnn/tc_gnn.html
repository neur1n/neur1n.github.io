<!DOCTYPE HTML>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1">

  <title>Jihang Li - TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</title>

  <link rel="stylesheet" type="text/css" href="/css/style.css">

  <script src="/js/mathjax/tex-chtml.js" async></script>
  <script src="/js/theme.js"></script>

  <script>
    listenToggle();
  </script>
</head>

<body>
  <object type="text/html" data="/header.html" class="header"></object>

  <div>
    <h1>TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</h1>

    <script>
      document.write(document.lastModified);
    </script>

    <ul>
      <li>
        <a href="#ten-questions">1 Ten Questions</a>
        <ul>
          <li>
            <a href="#q-1-1">1.1 What is the problem addressed in the paper?</a>
          </li>
          <li>
            <a href="#q-1-2">1.2 Is This a New Problem?</a>
          </li>
          <li>
            <a href="#q-1-3">1.3 What is the Scientific Hypothesis That the
              Paper is Trying to Verify?</a>
          </li>
          <li>
            <a href="#q-1-4">1.4 What are the key related works and who are the
              key people working on this topic?</a>
          </li>
          <li>
            <a href="#q-1-5">1.5 What is the Key of the Proposed Solution in
              the Paper?</a>
          </li>
          <li>
            <a href="#q-1-6">1.6 How are the Experiments Designed?</a>
          </li>
          <li>
            <a href="#q-1-7">1.7 What Datasets are Built/Used for the
              Quantitative Evaluation? Is the Code Open Sourced?</a>
          </li>
          <li>
            <a href="#q-1-8">1.8 Is the Scientific Hypothesis Well Supported by
              Evidence in the Experiments?</a>
          </li>
          <li>
            <a href="#q-1-9">1.9 What are the Contributions of the Paper?</a>
          </li>
          <li>
            <a href="#q-1-10">1.10 What Should/Could be Done Next?</a>
          </li>
        </ul>
      </li>
    </ul>

    <h2 id="ten-questions">1 Ten Questions</h2>
    <h3 id="q-1-1">1.1 What is the problem addressed in the paper?</h3>
    <p>
      Graph neural networks (GNNs) are known to have performance limitations,
      especially when dealing with <b>highly sparse and irregular graph-based</b>
      operations. The paper aims to improve the performance of GNNs in such
      scenarios by introducing TC-GNN, a framework tailored to utilize GPU
      Tensor Core Units (TCUs).
    </p>

    <h3 id="q-1-2">1.2 Is This a New Problem?</h3>
    <p>
      NVIDIA's <a href="https://developer.nvidia.com/cusparse"
        target="_blank">cuSPARSE</a> library provides basic linear algebra for
      sparse matrices. However, cuSPARSE's algorithm involves lots of high-cost
      <b>indirect memory accesses</b> on non-zero elements of a sparse matrix.
      Therefore, cuSPARSE cannot enjoy the same level of optimizations (e.g.,
      <b>data reuse</b>) as its dense counterpart, such as
      <a href="https://developer.nvidia.com/cublas" target="_blank">cuBLAS</a>.
    </p>

    <h3 id="q-1-3">1.3 What is the Scientific Hypothesis That the Paper is
      Trying to Verify?</h3>
    <p>
      The work focuses on exploring the potential of TCUs for accelerating
      GNN-based graph learning and suggests that the design and optimization
      principles may have broader applicability to other similar AI hardware
      (e.g., Google TPU and AMD Matrix Core) for sparse deep-learning workloads.
    </p>

    <h3 id="q-1-4">1.4 What are the key related works and who are the key
      people working on this topic?</h3>
    <p>
    </p>

    <h3 id="q-1-5">1.5 What is the Key of the Proposed Solution in the Paper?</h3>
    <p>
    </p>

    <h3 id="q-1-6">1.6 How are the Experiments Designed?</h3>
    <ol>
      <li>
        SpMM on CUDA cores
      </li>
    </ol>

    <h3 id="q-1-7">1.7 What Datasets are Built/Used for the Quantitative
      Evaluation? Is the Code Open Sourced?</h3>
    <p>
    </p>

    <h3 id="q-1-8">1.8 Is the Scientific Hypothesis Well Supported by Evidence
      in the Experiments?</h3>
    <p>
    </p>

    <h3 id="q-1-9">1.9 What are the Contributions of the Paper?</h3>
    <ol>
      <li>
        Conduct a detailed analysis of existing solutions (e.g., SpMM on CUDA
        cores) and identify the potentials of TCUs for accelerating sparse
        GNN workloads.
      </li>
      <li>
        Introduce a sparse graph translation technique. It can make the sparse
        and irregular GNN input graphs easily fit the dense computing of TCUs
        for acceleration.
      </li>
      <li>
        Build a TCU-tailored algorithm and GPU kernel design for CUDA core and
        TCU collaboration on GPUs to handle different sparse GNN computation.
      </li>
      <li>
        Extensive experiments show TC-GNN achieves 1.70Ã— speedup on average
        over the state-of-the-art GNN computing framework, Deep Graph Library,
        across various mainstream GNN models and dataset settings.
      </li>
    </ol>

    <h3 id="q-1-10">1.10 What Should/Could be Done Next?</h3>
    <ol>
      <li>
        Graph Partitioning/Reordering
        <ol>
          <li>
            <a href="https://proceedings.mlsys.org/paper_files/paper/2020/hash/91fc23ceccb664ebb0cf4257e1ba9c51-Abstract.html"
               target="_blank">ROC</a> introduces a <b>learning-based graph
            partitioning</b> to reduce the data movement between CPU and GPU
            when processing large graphs.
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/document/7515998"
              target="_blank">Rabbit Order</a> and
            <a href="https://dl.acm.org/doi/10.1145/800195.805928"
              target="_blank">Reverse Cuthill Mckee Algorithm</a>
            are focusing on <b>row reordering/clustering</b> to improve
            node/rowwise computation locality.
          </li>
          <li>
            Our sparse-graph translation (SGT) technique is <b>orthogonal and
            complementary</b> to these graph partitioning and reordering
            techniques since our SGT focuses on <b>column (neighbor)
            re-indexing</b> to improve neighbor-wise locality for TCU
            computation.
          </li>
        </ol>
      </li>
      <li>
        Distributed GNN Computation
        <ol>
          <li>
            <b>Distributed sampled graphs</b> (where graph nodes and their
            embeddings are on the same GPU): TC-GNN can be incorporated
            directly since all sampled graphs along with their node embeddings
            are presented at the same GPU.
          </li>
          <li>
            <b>Distributed full-graph</b> (where graph nodes and their
            embeddings may be on different GPUs): TC-GNN needs to be modified
            slightly by incorporating inter-GPU communication techniques (e.g.,
            <a href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/"
              target="_blank">Unified Virtual Memory</a>
            and <a href="https://developer.nvidia.com/nvshmem"
              target="_blank">NVSHMEM</a>)
            to support the remote neighbor embedding access.
          </li>
        </ol>
      </li>
    </ol>
  </div>
</body>
