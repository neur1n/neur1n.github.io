<!DOCTYPE HTML>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1">

  <title>Jihang Li - TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</title>

  <link rel="stylesheet" type="text/css" href="/css/style.css">

  <script src="/js/mathjax/tex-chtml.js" async></script>
  <script src="/js/theme.js"></script>

  <script>
    listenToggle();
  </script>
</head>

<body>
  <object type="text/html" data="/header.html" class="header"></object>

  <div>
    <h1>TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</h1>

    <p><em>2023-10-10 21:01</em></p>

    <ul>
      <li>
        <a href="#q-1">1 What is the Problem Addressed in the Paper?</a>
      </li>
      <li>
        <a href="#q-2">2 Is This a New Problem?</a>
      </li>
      <li>
        <a href="#q-3">3 What is the Scientific Hypothesis That the Paper is
          Trying to Verify?</a>
      </li>
      <li>
        <a href="#q-4">4 What are the Key Related Works and Who are the Key
          People Working on this Topic?</a>
      </li>
      <li>
        <a href="#q-5">5 What is the Key of the Proposed Solution in the Paper?</a>
      </li>
      <li>
        <a href="#q-6">6 How are the Experiments Designed?</a>
      </li>
      <li>
        <a href="#q-7">7 What Datasets are Built/Used for the Quantitative
          Evaluation? Is the Code Open Sourced?</a>
      </li>
      <li>
        <a href="#q-8">8 Is the Scientific Hypothesis Well Supported by
          Evidence in the Experiments?</a>
      </li>
      <li>
        <a href="#q-9">9 What are the Contributions of the Paper?</a>
      </li>
      <li>
        <a href="#q-10">10 What Should/Could be Done Next?</a>
      </li>
    </ul>

    <h2 id="q-1">1 What is the Problem Addressed in the Paper?</h2>
    <p>
      Graph neural networks (GNNs) are known to have performance limitations,
      especially when dealing with <b>highly sparse and irregular graph-based</b>
      operations. The paper aims to improve the performance of GNNs in such
      scenarios by introducing TC-GNN, a framework tailored to utilize GPU
      Tensor Core Units (TCUs).
    </p>

    <h2 id="q-2">2 Is This a New Problem?</h2>
    <p>
      NVIDIA's <a href="https://developer.nvidia.com/cusparse"
        target="_blank">cuSPARSE</a> library provides basic linear algebra for
      sparse matrices. However, cuSPARSE's algorithm involves lots of high-cost
      <b>indirect memory accesses</b> on non-zero elements of a sparse matrix.
      Therefore, cuSPARSE cannot enjoy the same level of optimizations (e.g.,
      <b>data reuse</b>) as its dense counterpart, such as
      <a href="https://developer.nvidia.com/cublas" target="_blank">cuBLAS</a>.
    </p>

    <h2 id="q-3">3 What is the Scientific Hypothesis That the Paper is Trying
      to Verify?</h2>
    <p>
      The pattern of the graph sparsity can be well-tuned for TCU computation
      through effective graph structural manipulation meanwhile guaranteeing
      output correctness.
    </p>

    <h2 id="q4">4 What are the Key Related Works and Who are the Key People
      Working on this Topic?</h2>
    <ul>
      <li>
        CUDA: Scalar computation.
      </li>
      <li>
        TCU: Tile-based matrix-matrix computation, which can deliver more than
        10× throughput improvement.
      </li>
      <li>
        Problem of working with sparse matrices:
      </li>
    </ul>

    <h2 id="q-5">5 What is the Key of the Proposed Solution in the Paper?</h2>
    <p>
      Prior efforts use TCUs in the <b>dense deep learning applications</b>
      that TCU is initially designed for, while TC-GNN jumps out of the scope
      defined by TCU designers and accelerates the <b>sparse full-graph GNNs</b>
      using TCUs.
    </p>

    <ol>
      <li>
        <b>TCU-aware Sparse Graph Translation (SGT)</b>: Condensing/remapping
        the neighbor indices into new neighbor indices that can facilitate the
        dense TCU computation paradigm. This fits the non-zero elements into
        fewer TCU blocks, thus reduces the unnecessary computation and memory
        overhead.

        <img src="/content/notes/gnn/tc_gnn/image/figure_4.png" alt="figure 4" style="width:80%">

        <img src="/content/notes/gnn/tc_gnn/image/algo_1.png" alt="algo 1" style="width:40%">
      </li>
      <li>
        <b>TCU-tailored GNN Computation</b>: Initiating tiles for TCU.
        <ul>
          <li>
            Neighbor Aggregation (for nodes): Focusing on maximizing the <b>net
            performance gains</b> by batching the originally highly irregular
            SpMM as dense GEMM computation and solving it on TCU.

            <img src="/content/notes/gnn/tc_gnn/image/algo_2.png" alt="algo 2" style="width:40%">
          </li>
          <li>
            Edge Feature Computing (for edges): Similar to neighbor aggregation,
            but with different outputs.

            <img src="/content/notes/gnn/tc_gnn/image/algo_3.png" alt="algo 3" style="width:40%">
          </li>
        </ul>
      </li>
      <li>
        <b>TCU-centric Workload Mapping</b>
        <ul>
          <li>
            GPU-aware Workload Decomposition
          </li>
          <li>
            TCU-optimized dataflow design
          </li>
        </ul>
      </li>
    </ol>

    <h2 id="q-6">6 How are the Experiments Designed?</h2>
    <ol>
      <li>
        SpMM on CUDA cores
      </li>
    </ol>

    <h2 id="q-7">7 What Datasets are Built/Used for the Quantitative
      Evaluation? Is the Code Open Sourced?</h2>
    <p>
    </p>

    <h2 id="q-8">8 Is the Scientific Hypothesis Well Supported by Evidence in
      the Experiments?</h2>
    <p>
    </p>

    <h2 id="q-9">9 What are the Contributions of the Paper?</h2>
    <ol>
      <li>
        Conduct a detailed analysis of existing solutions (e.g., SpMM on CUDA
        cores) and identify the potentials of TCUs for accelerating sparse
        GNN workloads.
      </li>
      <li>
        Introduce a sparse graph translation technique. It can make the sparse
        and irregular GNN input graphs easily fit the dense computing of TCUs
        for acceleration.
      </li>
      <li>
        Build a TCU-tailored algorithm and GPU kernel design for CUDA core and
        TCU collaboration on GPUs to handle different sparse GNN computation.
      </li>
      <li>
        Extensive experiments show TC-GNN achieves 1.70× speedup on average
        over the state-of-the-art GNN computing framework, Deep Graph Library,
        across various mainstream GNN models and dataset settings.
      </li>
    </ol>

    <h2 id="q-10">10 What Should/Could be Done Next?</h2>
    <ol>
      <li>
        Graph Partitioning/Reordering
        <ol>
          <li>
            <a href="https://proceedings.mlsys.org/paper_files/paper/2020/hash/91fc23ceccb664ebb0cf4257e1ba9c51-Abstract.html"
               target="_blank">ROC</a> introduces a <b>learning-based graph
            partitioning</b> to reduce the data movement between CPU and GPU
            when processing large graphs.
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/document/7515998"
              target="_blank">Rabbit Order</a> and
            <a href="https://dl.acm.org/doi/10.1145/800195.805928"
              target="_blank">Reverse Cuthill Mckee Algorithm</a>
            are focusing on <b>row reordering/clustering</b> to improve
            node/rowwise computation locality.
          </li>
          <li>
            Our sparse-graph translation (SGT) technique is <b>orthogonal and
            complementary</b> to these graph partitioning and reordering
            techniques since our SGT focuses on <b>column (neighbor)
            re-indexing</b> to improve neighbor-wise locality for TCU
            computation.
          </li>
        </ol>
      </li>
      <li>
        Distributed GNN Computation
        <ol>
          <li>
            <b>Distributed sampled graphs</b> (where graph nodes and their
            embeddings are on the same GPU): TC-GNN can be incorporated
            directly since all sampled graphs along with their node embeddings
            are presented at the same GPU.
          </li>
          <li>
            <b>Distributed full-graph</b> (where graph nodes and their
            embeddings may be on different GPUs): TC-GNN needs to be modified
            slightly by incorporating inter-GPU communication techniques (e.g.,
            <a href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/"
              target="_blank">Unified Virtual Memory</a>
            and <a href="https://developer.nvidia.com/nvshmem"
              target="_blank">NVSHMEM</a>)
            to support the remote neighbor embedding access.
          </li>
        </ol>
      </li>
    </ol>
  </div>
</body>
