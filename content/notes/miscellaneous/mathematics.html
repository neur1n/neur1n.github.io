<!DOCTYPE HTML>

<html lang="en"> <head>
  <meta charset="utf-8">

  <script src="/js/util.js"></script>

  <link rel="stylesheet" href="/css/header.css">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.4.1.min.js"></script>
  <script>
    $(function() {
      $("#header").load("/header.html")
    });
  </script>

  <script>window.MathJax = {tex: {tags: "ams"}};</script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <title>Jihang Li - Notes - Miscellaneous</title>
</head>

<body>
  <div id="header"></div>

  <div class="body-margin">
    <h1 style="text-align: center">Mathematics</h1>
    <script>lastUpdateDate()</script>
    <hr>

    <h2>Contents</h2>
    <ul>
      <li><a href="#distribution">Distribution</a></li>
      <ul>
        <li><a href="#normal-distribution">Normal Distribution</a></li>
        <li><a href="#poisson-distribution">Poisson Distribution</a></li>
      </ul>
      <li><a href="#probability">Probability</a></li>
      <ul>
        <li><a href="#normalizing-constant">Normalizing Constant</a></li>
      </ul>
    </ul>

    <!-- ============================================== Normal Distribution -->
    <hr class="part">
    <h1 style="text-align: center" id="distribution">Distribution</h1>
    <hr class="part">
    <!-- ============================================== Normal Distribution -->
    <h2 id="normal-distribution">Normal Distribution</h2>
    <h3>Basic Form</h3>
    <ul>
      <li>\(X\): random variable.</li>
      <li>\(\mu\): mean or expectation.</li>
      <li>\(\sigma\): standard deviation.</li>
      <li>\(\sigma^2\): variance.</li>
      <li>\(X \sim \mathcal{N}(\mu, \sigma^2)\): \(X\) is distributed normally
        with \(\mu\) and \(\sigma^2\),
        $$\begin{align} f(x \vert \mu, \sigma^2) = \frac{1}{2\sqrt \pi \sigma^2} \exp(-\frac{{(x - \mu)}^2}{2\sigma^2}). \end{align}$$
      </li>
    </ul>

    <h3>Multivariable Form</h3>
    <ul>
      <li>\(\mathbf{X}\): random vector, \(\mathbf{X} = (X_1, \ldots, X_k)\).</li>
      <li>\(\boldsymbol\mu\): mean or expectation vector, \(\boldsymbol\mu = \text{E}[\mathbf{X}] = (\text{E}[\mathbf{X_1}], \text{E}[\mathbf{X_2}], \ldots, \text{E}[\mathbf{X_k}])\).</li>
      <li>\(\boldsymbol\Sigma\): \(k \times k\) convariance matrix,
        \(\Sigma_{i,j} = \text{E}[(X_i - \mu_i)(X_j - \mu_j)] = \text{Cov}[X_i, X_j]\), where \(1 \leq i, j \leq k\).
      </li>
      <li>\(Q\): precision matrix \(\mathbf{\Sigma}^{-1}\).</li>
      <li>\(X \sim \mathcal{N}_k(\boldsymbol\mu, \boldsymbol\Sigma)\): \(X\) is
        distributed normally with \(\boldsymbol\mu\) and \(\boldsymbol\Sigma\)
        and \(k\) indicates \(k\)-dimension.
      </li>
    </ul>
    <p>
      For <strong>non-degenerate case</strong>:

      $$\begin{align} f_{\mathbf{X}}(x_1, \ldots, x_k) = \frac{1}{\sqrt{{(2\pi)}^k |\boldsymbol\Sigma|}} \exp(- \frac{{(\mathbf{X} - \boldsymbol\mu)}^{\text{T}} \boldsymbol\Sigma^{-1} (\mathbf{X} - \boldsymbol\mu)}{2}) \end{align}$$

      The descriptive statistic \(\sqrt{{(\mathbf{X} - \boldsymbol\mu)}^{\text{T}} \boldsymbol\Sigma^{-1} (\mathbf{X} - \boldsymbol\mu)}\)
      is known as the <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance" target="_blank">Mahalanobis distance</a>.
    </p>

    <!-- ============================================= Poisson Distribution -->
    <h2 id="poisson-distribution">Poisson Distribution</h2>

    <!-- ====================================================== Probability -->
    <hr class="part">
    <h1 style="text-align: center" id="probability">Probability</h1>
    <hr class="part">
    <!-- ============================================= Normalizing Constant -->
    <h2 id="normalizing-constant">Normalizing Constant</h2>
    <h3>Definition and Examples</h3>
    <p>
      In <a href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank">probability theory</a>,
      a normalizing constant is a constant by which an
      <strong>everywhere non-negative</strong> function must be multiplied so
      the area under its graph is 1, e.g., to make it a
      <a href="https://en.wikipedia.org/wiki/Probability_density_function" target="_blank">probability density function</a>
      or a <a href="https://en.wikipedia.org/wiki/Probability_mass_function" target="_blank">probability mass function</a>. For example, if we define

      $$\begin{align*} p(x)=e^{-x^{2}/2},x\in (-\infty, \infty) \end{align*}$$

      we have

      $$\begin{align*} \int_{-\infty}^{\infty}p(x) dx=\int_{-\infty}^{\infty} e^{-x^{2}/2} dx = {\sqrt {2\pi}} \end{align*}$$

      if we define a function \(\varphi(x)\) as

      $$\begin{align*} \varphi(x) = {\frac{1}{\sqrt {2\pi}}} p(x) = \frac{1}{\sqrt {2\pi}} e^{-x^{2}/2} \end{align*}$$

      So that

      $$\begin{align*} \int_{-\infty}^{\infty} \varphi(x) dx = \int_{-\infty}^{\infty} \frac{1}{\sqrt {2\pi}} e^{-x^{2}/2} dx = 1 \end{align*}$$

      then then function \(\varphi(x)\) is a probability density function. This
      is the density of the standard normal distribution. And constant
      \(\frac{1}{\sqrt {2\pi}}\) is the <strong>normalizing constant</strong>
      of function \(\varphi(x)\).
    </p>

    <p>
      Similarly,

      $$\begin{align*} \sum _{n=0}^{\infty }{\frac {\lambda ^{n}}{n!}}=e^{\lambda } \end{align*}$$

      and consequently

      $$\begin{align*} f(n)={\frac {\lambda ^{n}e^{-\lambda }}{n!}} \end{align*}$$

      is a probability mass function on the set of all nonnegative integers.
      This is the probability mass function of the Poisson distribution with
      expected value \(\lambda\). 
    </p>

    <h3>Bayes' Theorem</h3>
    <p>
      Bayes' theorem says that the posterior probability measure is
      proportional to the product of the prior probability measure and the
      likelihood function. <em>Proportional to</em> implies that one must
      multiply or divide by a normalizing constant to assign measure 1 to the
      whole space, i.e., to get a probability measure. In a simple discrete
      case we have

      $$\begin{align*} P(H_{0}|D)={\frac {P(D|H_{0})P(H_{0})}{P(D)}} \end{align*}$$

      where:
    </p>
    
    <ul>
      <li>
        \(P(H_0)\): the <strong>prior probability</strong> that the hypothesis
        is true
      </li>
      <li>
        \(P(D|H_0)\): the <strong>conditional probability</strong> of the data
        given that the hypothesis is true, but given that the data are known it
        is the likelihood of the hypothesis (or its parameters) given the data
      </li>
      <li>
        \(P(H_0|D)\): the <strong>posterior probability</strong> that the
        hypothesis is true given the data is true
      </li>
      <li>
        \(P(D)\): should be the probability of producing the data, but on its
        own is difficult to calculate, so an alternative way to describe this
        relationship is as one of proportionality:

        $$\begin{align*} P(H_{0}|D)\propto P(D|H_{0})P(H_{0}) \end{align*}$$
      </li>
    </ul>

    <p>
      Since \(P(H|D)\) is a probability, the sum over all possible (mutually
      exclusive) hypotheses should be 1, leading to the conclusion that:

      $$\begin{align*} P(H_0|D) = \frac{P(D|H_0)P(H_{0})}{\sum_i P(D|H_i)P(H_i)} \end{align*}$$

      In this case, the reciprocal of the value

      $$\begin{align*} P(D)=\sum _{i}P(D|H_{i})P(H_{i}) \end{align*}$$

      is the normalizing constant. It can be extended from countably many
      hypotheses to uncountably many by replacing the sum by an integral.
    </p>
  </div>
</body>
</html>
